{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73f56fce",
   "metadata": {},
   "source": [
    "# Feed Listeners\n",
    "\n",
    "## Overview\n",
    "This notebook explores using LLMs (and other AI techniques) to listen to and analyse feeds of information, which in this case is a BBC News RSS feed. The goal is to evaluate whether AI can filter out noisy articles and identify those that are relevant to a specific organisation or department.\n",
    "\n",
    "### The Experiment\n",
    "The core hypothesis under test is that an LLM and/or a combination of AI techniques can effectively filter a news feed to identify articles that are relevant to a specific organisation or department, while ignoring irrelevant content by:\n",
    "- Using an LLM to assess the relevance of each article based on its title and summary, and filtering out those that do not meet a certain relevance threshold.\n",
    "- Applying embedding techniques to compare the content of the articles with a predefined set of keywords or topics related to the organisation or department, and filtering out those that do not have a strong semantic similarity.\n",
    "- Analysing the sentiment of the articles to determine if they are positive, negative, or neutral towards the organisation or department in question.\n",
    "\n",
    "### What is in scope?\n",
    "The experiment will focus on the following aspects:\n",
    "- Ingesting a news feed ([BBC News Front Page RSS](http://feeds.bbci.co.uk/news/rss.xml) in this case)\n",
    "- Using and evaluating LLMs to filter the feed for relevance to a specific organisation or department\n",
    "- Evaluating the sustainability of using LLMs for this purpose, including computational resources and cost considerations. Can other AI techniques be used to reduce or complement the use of LLMs to make the process more efficient?\n",
    "- Do dedicated sentiment analysis models perform better than LLMs for sentiment analysis in this context? For example, Amazon Comphrehend, Azure Language, or Hugging Face sentiment analysis models?\n",
    "\n",
    "### What is out of scope?\n",
    "To keep the experiment focused and manageable, the following aspects are out of scope:\n",
    "- Production-grade streaming system architecture, scaling, and performance tuning.\n",
    "\n",
    "### Example System Context\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    DataSources[\"News Feeds & Data Sources<br/>(e.g., BBC News RSS, Reuters)\"]\n",
    "    FeedListener[\"Feed Listener System<br/>(Ingests, filters & analyses)\"]\n",
    "    Bedrock[\"LLM Provider<br/>(Amazon Bedrock / Azure AI Foundry)\"]\n",
    "    Sentiment[\"Sentiment Analysis<br/>(Amazon Comprehend / Azure Language / Dedicated Models)\"]\n",
    "    Analyst[\"Analyst<br/>(Consumes insights)\"]\n",
    "\n",
    "    DataSources -->|\"Provides articles<br/>(RSS/API)\"| FeedListener\n",
    "    FeedListener <-->|\"Assess relevance &<br/>generate embeddings\"| Bedrock\n",
    "    FeedListener <-->|\"Analyse sentiment\"| Sentiment\n",
    "    FeedListener -->|\"Filtered articles<br/>& insights\"| Analyst\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa1608c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Environment Setup\n",
    "We use `python-dotenv` to manage environment variables for this experiment, which loads variables from `notebooks/.env`. For instructions on setting up your environment along with prerequisites, please refer to the [README.md](../README.md) file in the root of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94dd0a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(dotenv_path=\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cec579e",
   "metadata": {},
   "source": [
    "## Source Data\n",
    "Our source dataset for this experiment is a BBC News RSS feed from the evening of 09 February 2026. This is located at `data/bbc_news_rss_feed_2026-02-09.xml` in this repository. The feed contains the BBC News front page articles, which we will use to test our feed listener system. Each article in the feed includes a title, summary, publication date, and a link to the full article.\n",
    "\n",
    "> [!IMPORTANT] - As the RSS feed is returned with no filter, it is possible some articles might cover a sensitive topic. We found during the course of this experiment that sometimes these types of articles can trigger LLM-level guardrails, which means the LLM will refuse to process the article and return an error instead. This is something to be aware of when working with real-world data feeds, as it can impact the performance and reliability of your feed listener system.\n",
    "> For the purposes of this experiment, we have removed any articles that triggered guardrails from the dataset, but in a production system you would need to consider how to handle such cases, whether that's by implementing additional filtering, using a different LLM provider with less restrictive guardrails, or by having a fallback mechanism in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb23936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import pydantic\n",
    "\n",
    "\n",
    "class RssFeedEntry(pydantic.BaseModel):\n",
    "    title: str\n",
    "    link: str\n",
    "    published: str\n",
    "    summary: str\n",
    "\n",
    "\n",
    "# BBC News Front Page RSS\n",
    "bbc_rss_path = \"./data/2026-02-09-bbc-news-rss.xml\"\n",
    "\n",
    "bbc_feed = feedparser.parse(bbc_rss_path)\n",
    "\n",
    "articles = [RssFeedEntry(**entry) for entry in bbc_feed.entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1e606f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pydantic_ai\n",
    "from pydantic_ai.embeddings import openai as pydantic_ai_openai_embeddings\n",
    "from pydantic_ai.models import openai as pydantic_ai_openai_models\n",
    "from pydantic_ai.providers import azure as pydantic_ai_azure_providers\n",
    "\n",
    "openai_client = openai.AsyncOpenAI(\n",
    "    base_url=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "gpt5_mini = pydantic_ai_openai_models.OpenAIChatModel(\n",
    "    model_name=\"gpt-5-mini\",\n",
    "    provider=pydantic_ai_azure_providers.AzureProvider(openai_client=openai_client),\n",
    ")\n",
    "\n",
    "ada_embeddings = pydantic_ai_openai_embeddings.OpenAIEmbeddingModel(\n",
    "    model_name=\"text-embedding-ada-002\",\n",
    "    provider=pydantic_ai_azure_providers.AzureProvider(openai_client=openai_client),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e991c2",
   "metadata": {},
   "source": [
    "## First Pass - LLM Relevance Filtering\n",
    "For the first pass we use an LLM to judge each article’s relevance from its title and summary. The model is prompted with a concise set of keywords, topics, or targeted questions about the organisation or department, and asked to return a binary Yes/No indicating whether the article is relevant.\n",
    "\n",
    "> [!IMPORTANT]\n",
    "> we avoid numeric relevance scores (e.g., 1–10) because LLMs can produce inconsistent or hallucinatory scales; requesting a binary classification reduces that risk and makes filtering straightforward.\n",
    "> \n",
    "> If finer granularity is needed, a dedicated classifier or a hybrid approach (e.g., embeddings + similarity scoring or a supervised ranking model) will likely be more suitable and reliable than asking an LLM for a relevance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c229a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Deps:\n",
    "    keywords: list[str]\n",
    "    topics: list[str]\n",
    "    questions: list[str]\n",
    "\n",
    "\n",
    "agent = pydantic_ai.Agent(deps_type=Deps, output_type=bool)\n",
    "\n",
    "\n",
    "@agent.system_prompt\n",
    "def system_prompt(ctx: pydantic_ai.RunContext[Deps]) -> str:\n",
    "    return f\"\"\"\n",
    "    You are an intelligent assistant designed to filter news articles for relevance to a specific organization or department.\n",
    "    Your task is to determine whether each article is relevant based on its title and summary, in relation to the following context:\n",
    "\n",
    "    Keywords: {\", \".join(ctx.deps.keywords)}\n",
    "    Topics: {\", \".join(ctx.deps.topics)}\n",
    "    Questions: {\", \".join(ctx.deps.questions)}\n",
    "\n",
    "    For each article, you will receive its title and summary.\n",
    "    You should return \"Yes\" if the article is relevant to the organization/department based on the provided context, and \"No\" if it is not.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f51aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def llm_check_articles(\n",
    "    articles: list[RssFeedEntry],\n",
    "    agent: pydantic_ai.Agent[Deps, bool],\n",
    "    deps: Deps,\n",
    "    model: pydantic_ai_openai_models.BaseModel,\n",
    ") -> tuple[list[RssFeedEntry], list[RssFeedEntry]]:\n",
    "    \"\"\"Filter articles by relevance using sequential LLM calls to respect rate limits.\n",
    "\n",
    "    Returns:\n",
    "        tuple of (relevant_articles, not_relevant_articles) for easy comparison\n",
    "    \"\"\"\n",
    "    relevant_articles = []\n",
    "    not_relevant_articles = []\n",
    "\n",
    "    for article in articles:\n",
    "        print(\"Checking article:\", article.title)\n",
    "\n",
    "        try:\n",
    "            result = await agent.run(\n",
    "                deps=deps,\n",
    "                model=model,\n",
    "                user_prompt=f\"Title: {article.title}\\nSummary: {article.summary}\\nIs this article relevant?\",\n",
    "            )\n",
    "\n",
    "            if result.output:\n",
    "                relevant_articles.append(article)\n",
    "            else:\n",
    "                not_relevant_articles.append(article)\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking article '{article.title}': {e}\")\n",
    "\n",
    "    return relevant_articles, not_relevant_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d82bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "defra_deps = Deps(\n",
    "    keywords=[\"uk government\", \"parliament\"],\n",
    "    topics=[\"politics\", \"government policy\", \"flooding\"],\n",
    "    questions=[\n",
    "        \"Is the article about the UK government or its policies?\",\n",
    "        \"Does the article discuss political events or decisions that could impact Defra?\",\n",
    "        \"Is the article relevant to current political issues in the UK?\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c61ce041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking article: Cabinet ministers rally round PM as Sarwar calls for him to quit\n",
      "Checking article: Scottish Labour leader makes his biggest political gamble - but can it pay off?\n",
      "Checking article: 'No regrets' - Vonn sustains 'complex tibia fracture'\n",
      "Checking article: Savannah Guthrie pleads for help as missing mother's ransom deadline looms\n",
      "Checking article: BBC assesses weaponry used to massacre Iran's protesters\n",
      "Checking article: Streeting's Mandelson messages reveal election fears and criticism of government\n",
      "Checking article: Palestinians say new Israeli measures in West Bank amount to de facto annexation\n",
      "Checking article: £5bn council SEND debts to be paid off by government\n",
      "Checking article: Family of Hillsborough victims criticise 'hollow' police apology\n",
      "Checking article: Airport safety concerns ground flights from remote British overseas island of St Helena\n",
      "Checking article: So close to a 'world first' - fourth for Brookes on frustrating day for Team GB\n",
      "Checking article: Olympic bosses investigate why medals are breaking\n",
      "Checking article: Ukraine skeleton racer says war victim helmet banned\n",
      "Checking article: GB in curling medal match - Tuesday's guide\n",
      "Checking article: Celebrity appearances, controversial ads and other Super Bowl takeaways\n",
      "Checking article: Businesses face extinction unless they protect nature, major report warns\n",
      "Checking article: The tech firms embracing a 72-hour working week\n",
      "Checking article: My songs get tens of thousands of likes but each Spotify stream earns me just £0.003\n",
      "Checking article: Late goals are destroying Liverpool's season and the answers aren't obvious\n",
      "Checking article: More than 90 flood warnings in place across UK as forecasters warn 'no sign' of dry spell\n",
      "Checking article: Extreme cold in New York City leaves 18 dead\n",
      "Checking article: Iran arrests reformists as crackdown on dissent widens, reports say\n",
      "Checking article: US boards tanker in Indian Ocean it 'tracked and hunted' from Caribbean\n",
      "Checking article: Friends set for new UK streamer as HBO Max reveals launch plans\n",
      "Checking article: BBC News app\n",
      "Checking article: Children in Care: How to Fix the Fostering Crisis\n",
      "Checking article: So close to a 'world first' - fourth for Brookes on frustrating day for Team GB\n",
      "Checking article: 'No regrets' - Vonn sustains 'complex tibia fracture'\n",
      "Checking article: Serena Williams can return from 22 February - but will she?\n",
      "Checking article: Darnold's Super Bowl win - the ultimate redemption story?\n",
      "Checking article: Ukraine skeleton racer claims war victim helmet banned by IOC\n",
      "Checking article: 'She had gold in her hands!' - GB's Brookes' costly landing on huge final jump\n"
     ]
    }
   ],
   "source": [
    "gpt5_relevant, gpt5_not_relevant = await llm_check_articles(\n",
    "    articles, agent, defra_deps, gpt5_mini\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31763899",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "295b1d02",
   "metadata": {},
   "source": [
    "## First Pass Results\n",
    "As expected, the LLM was more than capable of filtering articles for relevance according to the criteria we set. However, we found that it was not very efficient at this task, as it required a call to the LLM for each article in the feed, which quickly adds up in terms of cost and latency. Most of the articles were filtered out as not relevant, which is good in terms of accuracy, but it means we made a lot of LLM calls to achieve that result.\n",
    "\n",
    "### Side Note - Structured Output with Pydantic AI\n",
    "As an interesting side note - During this experiment we found out how Pydantic AI implements LLM's outputting to it's output type. As a reminder, one of the main selling points of the framework is that it can output structured data types according to a Pydantic schema or basic types. In this case, we wanted the LLM to output a boolean value indicating relevance, so we just specified the output type as `bool` in the `Agent` definition.\n",
    "\n",
    "We found that when using GPT-5, the model consistently returned a boolean value as expected, which made it very easy to filter the articles based on relevance and avoid any additional parsing or error handling. However, when we switched to Phi-4, we found that the model API returned an error with our request due to `ToolConfig`. After some investigation, we found that this was because Phi-4 does not support tool calling at all. This led us down the path of discovering Pydantic AI registers a tool that handles enforcing the output type(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18200203",
   "metadata": {},
   "source": [
    "## Second Pass - Embedding-Based Relevance Filtering\n",
    "\n",
    "Instead of using an LLM to assess relevance, we can use embeddings to calculate semantic similarity between articles and our relevance criteria. This approach:\n",
    "- Is more efficient (no LLM inference needed for filtering)\n",
    "- Is more cost-effective (embeddings are cheaper than LLM calls)\n",
    "- Provides numeric similarity scores for fine-grained filtering\n",
    "- Is more sustainable for high-volume feeds\n",
    "\n",
    "We'll use cosine similarity to compare article embeddings against embeddings of our keywords, topics, and questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44607f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "from pydantic_ai.embeddings import Embedder\n",
    "\n",
    "\n",
    "def cosine_similarity(vec1: list[float], vec2: list[float]) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    a = np.array(vec1)\n",
    "    b = np.array(vec2)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ArticleRelevanceScore:\n",
    "    \"\"\"Detailed relevance breakdown for an article.\"\"\"\n",
    "\n",
    "    article: RssFeedEntry\n",
    "    keyword_scores: dict[str, float]\n",
    "    topic_scores: dict[str, float]\n",
    "    question_scores: dict[str, float]\n",
    "    avg_keyword_score: float\n",
    "    avg_topic_score: float\n",
    "    avg_question_score: float\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ArticleRelevanceScore(title='{self.article.title[:30]}...', keywords={self.avg_keyword_score:.3f}, topics={self.avg_topic_score:.3f}, questions={self.avg_question_score:.3f})\"\n",
    "\n",
    "\n",
    "async def embedding_check_articles(\n",
    "    articles: list[RssFeedEntry],\n",
    "    deps: Deps,\n",
    "    embedding_model: pydantic_ai_openai_embeddings.OpenAIEmbeddingModel,\n",
    "    relevance_threshold: float = 0.75,\n",
    "    weights: dict[str, float] | None = None,\n",
    ") -> tuple[list[ArticleRelevanceScore], list[ArticleRelevanceScore]]:\n",
    "    \"\"\"Filter articles by relevance using embedding similarity with granular scoring.\n",
    "\n",
    "    This approach compares each article against keywords, topics, and questions individually,\n",
    "    providing detailed insight into what makes an article relevant.\n",
    "\n",
    "    Args:\n",
    "        articles: List of RSS feed entries to check\n",
    "        deps: Dependencies containing keywords, topics, and questions\n",
    "        embedding_model: The embedding model to use\n",
    "        relevance_threshold: Minimum overall score to consider relevant (0-1)\n",
    "        weights: Optional weights for combining scores. Defaults to equal weights.\n",
    "                Keys: 'keywords', 'topics', 'questions'\n",
    "\n",
    "    Returns:\n",
    "        tuple of (relevant_articles_with_scores, not_relevant_articles_with_scores)\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = {\"keywords\": 0.25, \"topics\": 0.25, \"questions\": 0.5}\n",
    "\n",
    "    embedder = Embedder(model=embedding_model)\n",
    "\n",
    "    print(\"Generating context embeddings...\")\n",
    "\n",
    "    keyword_embeddings = {}\n",
    "    if deps.keywords:\n",
    "        keyword_results = await embedder.embed_query(deps.keywords)\n",
    "        keyword_embeddings = dict(\n",
    "            zip(deps.keywords, keyword_results.embeddings, strict=True)\n",
    "        )\n",
    "        print(f\"  Embedded {len(deps.keywords)} keywords\")\n",
    "\n",
    "    topic_embeddings = {}\n",
    "    if deps.topics:\n",
    "        topic_results = await embedder.embed_query(deps.topics)\n",
    "        topic_embeddings = dict(zip(deps.topics, topic_results.embeddings, strict=True))\n",
    "        print(f\"  Embedded {len(deps.topics)} topics\")\n",
    "\n",
    "    question_embeddings = {}\n",
    "    if deps.questions:\n",
    "        question_results = await embedder.embed_query(deps.questions)\n",
    "        question_embeddings = dict(\n",
    "            zip(deps.questions, question_results.embeddings, strict=True)\n",
    "        )\n",
    "        print(f\"  Embedded {len(deps.questions)} questions\")\n",
    "\n",
    "    relevant_articles = []\n",
    "    not_relevant_articles = []\n",
    "\n",
    "    for article in articles:\n",
    "        article_text = f\"{article.title} {article.summary}\"\n",
    "        print(f\"\\nChecking article: {article.title[:50]}...\")\n",
    "\n",
    "        try:\n",
    "            article_result = await embedder.embed_documents(article_text)\n",
    "            article_embedding = article_result.embeddings[0]\n",
    "\n",
    "            keyword_scores = {\n",
    "                kw: cosine_similarity(emb, article_embedding)\n",
    "                for kw, emb in keyword_embeddings.items()\n",
    "            }\n",
    "\n",
    "            topic_scores = {\n",
    "                topic: cosine_similarity(emb, article_embedding)\n",
    "                for topic, emb in topic_embeddings.items()\n",
    "            }\n",
    "\n",
    "            question_scores = {\n",
    "                q: cosine_similarity(emb, article_embedding)\n",
    "                for q, emb in question_embeddings.items()\n",
    "            }\n",
    "\n",
    "            avg_keyword_score = (\n",
    "                sum(keyword_scores.values()) / len(keyword_scores)\n",
    "                if keyword_scores\n",
    "                else 0.0\n",
    "            )\n",
    "\n",
    "            avg_topic_score = (\n",
    "                sum(topic_scores.values()) / len(topic_scores) if topic_scores else 0.0\n",
    "            )\n",
    "            \n",
    "            avg_question_score = (\n",
    "                sum(question_scores.values()) / len(question_scores)\n",
    "                if question_scores\n",
    "                else 0.0\n",
    "            )\n",
    "\n",
    "            relevant = np.average(\n",
    "                [avg_keyword_score, avg_topic_score, avg_question_score],\n",
    "                weights=[weights[\"keywords\"], weights[\"topics\"], weights[\"questions\"]],\n",
    "            )\n",
    "\n",
    "            score = ArticleRelevanceScore(\n",
    "                article=article,\n",
    "                keyword_scores=keyword_scores,\n",
    "                topic_scores=topic_scores,\n",
    "                question_scores=question_scores,\n",
    "                avg_keyword_score=avg_keyword_score,\n",
    "                avg_topic_score=avg_topic_score,\n",
    "                avg_question_score=avg_question_score,\n",
    "            )\n",
    "\n",
    "            print(score)\n",
    "\n",
    "            if relevant >= relevance_threshold:\n",
    "                relevant_articles.append(article)\n",
    "            else:\n",
    "                not_relevant_articles.append(article)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error embedding article '{article.title}': {e}\")\n",
    "\n",
    "    return relevant_articles, not_relevant_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b15b1f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating context embeddings...\n",
      "  Embedded 2 keywords\n",
      "  Embedded 3 topics\n",
      "  Embedded 3 questions\n",
      "\n",
      "Checking article: Cabinet ministers rally round PM as Sarwar calls f...\n",
      "ArticleRelevanceScore(title='Cabinet ministers rally round ...', keywords=0.785, topics=0.727, questions=0.761)\n",
      "\n",
      "Checking article: Scottish Labour leader makes his biggest political...\n",
      "ArticleRelevanceScore(title='Scottish Labour leader makes h...', keywords=0.754, topics=0.721, questions=0.772)\n",
      "\n",
      "Checking article: 'No regrets' - Vonn sustains 'complex tibia fractu...\n",
      "ArticleRelevanceScore(title=''No regrets' - Vonn sustains '...', keywords=0.721, topics=0.723, questions=0.712)\n",
      "\n",
      "Checking article: Savannah Guthrie pleads for help as missing mother...\n",
      "ArticleRelevanceScore(title='Savannah Guthrie pleads for he...', keywords=0.732, topics=0.734, questions=0.728)\n",
      "\n",
      "Checking article: BBC assesses weaponry used to massacre Iran's prot...\n",
      "ArticleRelevanceScore(title='BBC assesses weaponry used to ...', keywords=0.761, topics=0.732, questions=0.754)\n",
      "\n",
      "Checking article: Streeting's Mandelson messages reveal election fea...\n",
      "ArticleRelevanceScore(title='Streeting's Mandelson messages...', keywords=0.782, topics=0.754, questions=0.784)\n",
      "\n",
      "Checking article: Palestinians say new Israeli measures in West Bank...\n",
      "ArticleRelevanceScore(title='Palestinians say new Israeli m...', keywords=0.742, topics=0.751, questions=0.733)\n",
      "\n",
      "Checking article: £5bn council SEND debts to be paid off by governme...\n",
      "ArticleRelevanceScore(title='£5bn council SEND debts to be ...', keywords=0.784, topics=0.751, questions=0.774)\n",
      "\n",
      "Checking article: Family of Hillsborough victims criticise 'hollow' ...\n",
      "ArticleRelevanceScore(title='Family of Hillsborough victims...', keywords=0.737, topics=0.720, questions=0.732)\n",
      "\n",
      "Checking article: Airport safety concerns ground flights from remote...\n",
      "ArticleRelevanceScore(title='Airport safety concerns ground...', keywords=0.755, topics=0.739, questions=0.750)\n",
      "\n",
      "Checking article: So close to a 'world first' - fourth for Brookes o...\n",
      "ArticleRelevanceScore(title='So close to a 'world first' - ...', keywords=0.747, topics=0.733, questions=0.735)\n",
      "\n",
      "Checking article: Olympic bosses investigate why medals are breaking...\n",
      "ArticleRelevanceScore(title='Olympic bosses investigate why...', keywords=0.736, topics=0.738, questions=0.746)\n",
      "\n",
      "Checking article: Ukraine skeleton racer says war victim helmet bann...\n",
      "ArticleRelevanceScore(title='Ukraine skeleton racer says wa...', keywords=0.755, topics=0.737, questions=0.734)\n",
      "\n",
      "Checking article: GB in curling medal match - Tuesday's guide...\n",
      "ArticleRelevanceScore(title='GB in curling medal match - Tu...', keywords=0.730, topics=0.731, questions=0.740)\n",
      "\n",
      "Checking article: Celebrity appearances, controversial ads and other...\n",
      "ArticleRelevanceScore(title='Celebrity appearances, controv...', keywords=0.683, topics=0.710, questions=0.715)\n",
      "\n",
      "Checking article: Businesses face extinction unless they protect nat...\n",
      "ArticleRelevanceScore(title='Businesses face extinction unl...', keywords=0.749, topics=0.752, questions=0.757)\n",
      "\n",
      "Checking article: The tech firms embracing a 72-hour working week...\n",
      "ArticleRelevanceScore(title='The tech firms embracing a 72-...', keywords=0.746, topics=0.744, questions=0.742)\n",
      "\n",
      "Checking article: My songs get tens of thousands of likes but each S...\n",
      "ArticleRelevanceScore(title='My songs get tens of thousands...', keywords=0.721, topics=0.715, questions=0.718)\n",
      "\n",
      "Checking article: Late goals are destroying Liverpool's season and t...\n",
      "ArticleRelevanceScore(title='Late goals are destroying Live...', keywords=0.706, topics=0.715, questions=0.739)\n",
      "\n",
      "Checking article: More than 90 flood warnings in place across UK as ...\n",
      "ArticleRelevanceScore(title='More than 90 flood warnings in...', keywords=0.748, topics=0.742, questions=0.751)\n",
      "\n",
      "Checking article: Extreme cold in New York City leaves 18 dead...\n",
      "ArticleRelevanceScore(title='Extreme cold in New York City ...', keywords=0.716, topics=0.740, questions=0.719)\n",
      "\n",
      "Checking article: Iran arrests reformists as crackdown on dissent wi...\n",
      "ArticleRelevanceScore(title='Iran arrests reformists as cra...', keywords=0.759, topics=0.747, questions=0.748)\n",
      "\n",
      "Checking article: US boards tanker in Indian Ocean it 'tracked and h...\n",
      "ArticleRelevanceScore(title='US boards tanker in Indian Oce...', keywords=0.752, topics=0.731, questions=0.731)\n",
      "\n",
      "Checking article: Friends set for new UK streamer as HBO Max reveals...\n",
      "ArticleRelevanceScore(title='Friends set for new UK streame...', keywords=0.747, topics=0.706, questions=0.720)\n",
      "\n",
      "Checking article: BBC News app...\n",
      "ArticleRelevanceScore(title='BBC News app...', keywords=0.765, topics=0.744, questions=0.773)\n",
      "\n",
      "Checking article: Children in Care: How to Fix the Fostering Crisis...\n",
      "ArticleRelevanceScore(title='Children in Care: How to Fix t...', keywords=0.736, topics=0.744, questions=0.747)\n",
      "\n",
      "Checking article: So close to a 'world first' - fourth for Brookes o...\n",
      "ArticleRelevanceScore(title='So close to a 'world first' - ...', keywords=0.747, topics=0.733, questions=0.735)\n",
      "\n",
      "Checking article: 'No regrets' - Vonn sustains 'complex tibia fractu...\n",
      "ArticleRelevanceScore(title=''No regrets' - Vonn sustains '...', keywords=0.721, topics=0.723, questions=0.712)\n",
      "\n",
      "Checking article: Serena Williams can return from 22 February - but ...\n",
      "ArticleRelevanceScore(title='Serena Williams can return fro...', keywords=0.710, topics=0.697, questions=0.714)\n",
      "\n",
      "Checking article: Darnold's Super Bowl win - the ultimate redemption...\n",
      "ArticleRelevanceScore(title='Darnold's Super Bowl win - the...', keywords=0.686, topics=0.692, questions=0.705)\n",
      "\n",
      "Checking article: Ukraine skeleton racer claims war victim helmet ba...\n",
      "ArticleRelevanceScore(title='Ukraine skeleton racer claims ...', keywords=0.752, topics=0.735, questions=0.732)\n",
      "\n",
      "Checking article: 'She had gold in her hands!' - GB's Brookes' costl...\n",
      "ArticleRelevanceScore(title=''She had gold in her hands!' -...', keywords=0.734, topics=0.729, questions=0.725)\n",
      "Relevant articles: 8\n",
      "Not relevant articles: 24\n"
     ]
    }
   ],
   "source": [
    "# Run embedding-based relevance check with the same DEFRA context\n",
    "# You can adjust weights to emphasize different components\n",
    "# e.g., weights={'keywords': 0.5, 'topics': 0.3, 'questions': 0.2}\n",
    "embedding_relevant, embedding_not_relevant = await embedding_check_articles(\n",
    "    articles,\n",
    "    defra_deps,\n",
    "    ada_embeddings,\n",
    "    relevance_threshold=0.75,  # Adjust this threshold based on results\n",
    ")\n",
    "\n",
    "print(f\"Relevant articles: {len(embedding_relevant)}\")\n",
    "print(f\"Not relevant articles: {len(embedding_not_relevant)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f5114",
   "metadata": {},
   "source": [
    "## Second Pass Results\n",
    "The embedding-based filtering was able to identify a similar set of relevant articles as the LLM, but with significantly fewer computational resources and at a much lower cost. By setting an appropriate relevance threshold, we were able to filter out a large portion of the irrelevant articles while retaining most of the relevant ones.\n",
    "\n",
    "However, we did find that the embedding-based approach was not perfect although it mostly (apart from one) captured the same relevant articles along with a few additional ones that the LLM said were not relevant. This could be due to that embeddings capture semantic similarity but do not have the same level of contextual understanding as an LLM, which can lead to some false positives or false negatives depending on the specific content of the articles and how well it matches the relevance criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5cbd0c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>LLM Agent (GPT-5)</th>\n",
       "      <th>Embedding Model</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Relevant Articles</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not Relevant Articles</td>\n",
       "      <td>28</td>\n",
       "      <td>24</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total Articles</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Agreed as Relevant (Both)</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Only LLM Found Relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Only Embedding Found Relevant</td>\n",
       "      <td>-</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Metric LLM Agent (GPT-5) Embedding Model Difference\n",
       "0              Relevant Articles                 4               8          4\n",
       "1          Not Relevant Articles                28              24         -4\n",
       "2                 Total Articles                32              32          0\n",
       "3                                                                            \n",
       "4      Agreed as Relevant (Both)                 3               3           \n",
       "5        Only LLM Found Relevant                 1               -           \n",
       "6  Only Embedding Found Relevant                 -               5           "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compare LLM Agent vs Embedding Model\n",
    "gpt5_total_relevant = len(gpt5_relevant)\n",
    "gpt5_total_not_relevant = len(gpt5_not_relevant)\n",
    "embedding_total_relevant = len(embedding_relevant)\n",
    "embedding_total_not_relevant = len(embedding_not_relevant)\n",
    "\n",
    "# Create sets of titles for comparison\n",
    "gpt5_relevant_titles = {article.title for article in gpt5_relevant}\n",
    "embedding_relevant_titles = {article.title for article in embedding_relevant}\n",
    "\n",
    "# Find differences\n",
    "both_relevant = gpt5_relevant_titles & embedding_relevant_titles\n",
    "only_gpt5 = gpt5_relevant_titles - embedding_relevant_titles\n",
    "only_embedding = embedding_relevant_titles - gpt5_relevant_titles\n",
    "\n",
    "# Create summary table\n",
    "summary_data = {\n",
    "    \"Metric\": [\n",
    "        \"Relevant Articles\",\n",
    "        \"Not Relevant Articles\",\n",
    "        \"Total Articles\",\n",
    "        \"\",\n",
    "        \"Agreed as Relevant (Both)\",\n",
    "        \"Only LLM Found Relevant\",\n",
    "        \"Only Embedding Found Relevant\",\n",
    "    ],\n",
    "    \"LLM Agent (GPT-5)\": [\n",
    "        gpt5_total_relevant,\n",
    "        gpt5_total_not_relevant,\n",
    "        gpt5_total_relevant + gpt5_total_not_relevant,\n",
    "        \"\",\n",
    "        len(both_relevant),\n",
    "        len(only_gpt5),\n",
    "        \"-\",\n",
    "    ],\n",
    "    \"Embedding Model\": [\n",
    "        embedding_total_relevant,\n",
    "        embedding_total_not_relevant,\n",
    "        embedding_total_relevant + embedding_total_not_relevant,\n",
    "        \"\",\n",
    "        len(both_relevant),\n",
    "        \"-\",\n",
    "        len(only_embedding),\n",
    "    ],\n",
    "    \"Difference\": [\n",
    "        embedding_total_relevant - gpt5_total_relevant,\n",
    "        embedding_total_not_relevant - gpt5_total_not_relevant,\n",
    "        0,\n",
    "        \"\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e76700",
   "metadata": {},
   "source": [
    "## Third Pass - Embedding into LLM Agent (Hybrid Approach)\n",
    "\n",
    "This hybrid approach combines the best of both methods:\n",
    "1. **Fast Pre-filtering with Embeddings**: Use semantic similarity to quickly filter out clearly irrelevant articles (cheap, fast, sustainable)\n",
    "2. **LLM Analysis on Filtered Subset**: Apply the LLM agent only to pre-filtered articles for contextual understanding and nuanced judgment\n",
    "\n",
    "Benefits:\n",
    "- **Cost-effective**: Reduces LLM calls by ~50-80% depending on the embedding threshold\n",
    "- **Sustainable**: Lower computational overhead and token usage\n",
    "- **High Quality**: LLM still provides contextual analysis where it matters most\n",
    "- **Scalable**: Can handle high-volume feeds efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99976ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking article: Cabinet ministers rally round PM as Sarwar calls for him to quit\n",
      "Checking article: Scottish Labour leader makes his biggest political gamble - but can it pay off?\n",
      "Checking article: BBC assesses weaponry used to massacre Iran's protesters\n",
      "Checking article: Streeting's Mandelson messages reveal election fears and criticism of government\n",
      "Checking article: £5bn council SEND debts to be paid off by government\n",
      "Checking article: Businesses face extinction unless they protect nature, major report warns\n",
      "Checking article: Iran arrests reformists as crackdown on dissent widens, reports say\n",
      "Checking article: BBC News app\n",
      "Embedding pre-filter: 8/32 articles\n",
      "LLM final filter: 4/8 articles\n",
      "Total LLM calls saved: 24 (75.0%)\n"
     ]
    }
   ],
   "source": [
    "# Apply LLM agent only to embedding-filtered articles\n",
    "# This reuses the embedding_relevant list from the second pass\n",
    "hybrid_relevant, hybrid_not_relevant = await llm_check_articles(\n",
    "    embedding_relevant,  # Only check pre-filtered articles\n",
    "    agent,\n",
    "    defra_deps,\n",
    "    gpt5_mini,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Embedding pre-filter: {len(embedding_relevant)}/{len(articles)} articles\"\n",
    ")\n",
    "print(f\"LLM final filter: {len(hybrid_relevant)}/{len(embedding_relevant)} articles\")\n",
    "print(\n",
    "    f\"Total LLM calls saved: {len(articles) - len(embedding_relevant)} ({(1 - len(embedding_relevant) / len(articles)) * 100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dad0993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>LLM Only</th>\n",
       "      <th>Embedding Only</th>\n",
       "      <th>Hybrid (Embedding→LLM)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Relevant Articles</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not Relevant Articles</td>\n",
       "      <td>28</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total Articles</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LLM Calls Made</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>All 3 Methods Agree (Relevant)</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LLM-Only &amp; Hybrid Agree</td>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Embedding-Only &amp; Hybrid Agree</td>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Efficiency (LLM Calls Saved)</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Percentage of LLM Calls Saved</td>\n",
       "      <td>0%</td>\n",
       "      <td>100%</td>\n",
       "      <td>75.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Metric LLM Only Embedding Only  \\\n",
       "0                Relevant Articles        4              8   \n",
       "1            Not Relevant Articles       28             24   \n",
       "2                   Total Articles       32             32   \n",
       "3                   LLM Calls Made       32              0   \n",
       "4                                                            \n",
       "5   All 3 Methods Agree (Relevant)        3              3   \n",
       "6          LLM-Only & Hybrid Agree        3              -   \n",
       "7    Embedding-Only & Hybrid Agree        -              4   \n",
       "8                                                            \n",
       "9     Efficiency (LLM Calls Saved)        0             32   \n",
       "10   Percentage of LLM Calls Saved       0%           100%   \n",
       "\n",
       "   Hybrid (Embedding→LLM)  \n",
       "0                       4  \n",
       "1                      28  \n",
       "2                      32  \n",
       "3                       8  \n",
       "4                          \n",
       "5                       3  \n",
       "6                       3  \n",
       "7                       4  \n",
       "8                          \n",
       "9                      24  \n",
       "10                  75.0%  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare all three approaches\n",
    "hybrid_total_relevant = len(hybrid_relevant)\n",
    "hybrid_total_not_relevant = len(hybrid_not_relevant) + len(\n",
    "    embedding_not_relevant\n",
    ")  # Articles rejected by embedding + LLM\n",
    "\n",
    "# Create sets for comparison\n",
    "hybrid_relevant_titles = {article.title for article in hybrid_relevant}\n",
    "\n",
    "# Find agreement patterns\n",
    "all_three_agree = (\n",
    "    gpt5_relevant_titles & embedding_relevant_titles & hybrid_relevant_titles\n",
    ")\n",
    "gpt5_hybrid_agree = gpt5_relevant_titles & hybrid_relevant_titles\n",
    "embedding_hybrid_agree = embedding_relevant_titles & hybrid_relevant_titles\n",
    "\n",
    "# Create comprehensive comparison table\n",
    "comparison_data = {\n",
    "    \"Metric\": [\n",
    "        \"Relevant Articles\",\n",
    "        \"Not Relevant Articles\",\n",
    "        \"Total Articles\",\n",
    "        \"LLM Calls Made\",\n",
    "        \"\",\n",
    "        \"All 3 Methods Agree (Relevant)\",\n",
    "        \"LLM-Only & Hybrid Agree\",\n",
    "        \"Embedding-Only & Hybrid Agree\",\n",
    "        \"\",\n",
    "        \"Efficiency (LLM Calls Saved)\",\n",
    "        \"Percentage of LLM Calls Saved\",\n",
    "    ],\n",
    "    \"LLM Only\": [\n",
    "        gpt5_total_relevant,\n",
    "        gpt5_total_not_relevant,\n",
    "        len(articles),\n",
    "        len(articles),\n",
    "        \"\",\n",
    "        len(all_three_agree),\n",
    "        len(gpt5_hybrid_agree),\n",
    "        \"-\",\n",
    "        \"\",\n",
    "        0,\n",
    "        \"0%\",\n",
    "    ],\n",
    "    \"Embedding Only\": [\n",
    "        embedding_total_relevant,\n",
    "        embedding_total_not_relevant,\n",
    "        len(articles),\n",
    "        0,\n",
    "        \"\",\n",
    "        len(all_three_agree),\n",
    "        \"-\",\n",
    "        len(embedding_hybrid_agree),\n",
    "        \"\",\n",
    "        len(articles),\n",
    "        \"100%\",\n",
    "    ],\n",
    "    \"Hybrid (Embedding→LLM)\": [\n",
    "        hybrid_total_relevant,\n",
    "        hybrid_total_not_relevant,\n",
    "        len(articles),\n",
    "        len(embedding_relevant),\n",
    "        \"\",\n",
    "        len(all_three_agree),\n",
    "        len(gpt5_hybrid_agree),\n",
    "        len(embedding_hybrid_agree),\n",
    "        \"\",\n",
    "        len(articles) - len(embedding_relevant),\n",
    "        f\"{(1 - len(embedding_relevant) / len(articles)) * 100:.1f}%\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7cd3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Articles where methods disagree ===\n",
      "\n",
      "LLM-Only found relevant but Hybrid rejected (1):\n",
      "  - More than 90 flood warnings in place across UK as forecasters warn 'no sign' of dry spell\n",
      "\n",
      "Hybrid found relevant but LLM-Only rejected (1):\n",
      "  - Scottish Labour leader makes his biggest political gamble - but can it pay off?\n",
      "\n",
      "All 3 methods agree these are relevant (3):\n",
      "  - £5bn council SEND debts to be paid off by government\n",
      "  - Streeting's Mandelson messages reveal election fears and criticism of government\n",
      "  - Cabinet ministers rally round PM as Sarwar calls for him to quit\n"
     ]
    }
   ],
   "source": [
    "# Analyze differences between approaches\n",
    "only_llm = gpt5_relevant_titles - hybrid_relevant_titles\n",
    "only_hybrid = hybrid_relevant_titles - gpt5_relevant_titles\n",
    "\n",
    "if only_llm:\n",
    "    print(f\"LLM-Only found relevant but Hybrid rejected ({len(only_llm)}):\")\n",
    "    for title in only_llm:\n",
    "        print(f\"  - {title}\")\n",
    "    print()\n",
    "\n",
    "if only_hybrid:\n",
    "    print(f\"Hybrid found relevant but LLM-Only rejected ({len(only_hybrid)}):\")\n",
    "    for title in only_hybrid:\n",
    "        print(f\"  - {title}\")\n",
    "    print()\n",
    "\n",
    "if all_three_agree:\n",
    "    print(f\"All 3 methods agree these are relevant ({len(all_three_agree)}):\")\n",
    "    for title in all_three_agree:\n",
    "        print(f\"  - {title}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-spike-intelligent-monitoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
